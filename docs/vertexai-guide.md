# üß† Guide Vertex AI - Magic Button

## Vue d'ensemble

Ce guide d√©taille l'utilisation de Vertex AI dans le projet Magic Button, couvrant l'API Gemini pour la g√©n√©ration de texte et l'API Text Embeddings pour la recherche s√©mantique (RAG).

## Configuration et authentification

### Service Account et permissions

```bash
# Service Account avec les r√¥les n√©cessaires
gcloud iam service-accounts create magic-button-api \
    --display-name="Magic Button API"

# Permissions Vertex AI
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:magic-button-api@$PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/aiplatform.user"
```

### SDK et initialisation

```typescript
import { VertexAI } from '@google-cloud/vertexai';

// Initialisation du client Vertex AI
const vertexAI = new VertexAI({
  project: process.env.PROJECT_ID,
  location: process.env.VERTEX_LOCATION, // us-central1
});

// Mod√®les disponibles
const generativeModel = vertexAI.getGenerativeModel({
  model: 'gemini-1.5-pro',
});

const embeddingModel = vertexAI.getGenerativeModel({
  model: 'text-embedding-004',
});
```

## API Gemini - G√©n√©ration de texte

### Configuration des param√®tres

```typescript
interface GenerationConfig {
  temperature: number;        // 0.0-2.0, cr√©ativit√©
  topP: number;              // 0.0-1.0, diversit√© tokens
  topK: number;              // Nombre top tokens
  maxOutputTokens: number;   // Limite r√©ponse
  candidateCount: number;    // Nombre alternatives
}

const generationConfig: GenerationConfig = {
  temperature: 0.2,          // Faible pour coh√©rence
  topP: 0.8,                 // Bon √©quilibre
  topK: 40,                  // Standard
  maxOutputTokens: 1024,     // R√©ponses courtes
  candidateCount: 1,         // Une seule r√©ponse
};
```

### Filtres de s√©curit√©

```typescript
import { HarmCategory, HarmBlockThreshold } from '@google-cloud/vertexai';

const safetySettings = [
  {
    category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_HARASSMENT,
    threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
  },
];
```

### Actions IA - Exemples concrets

#### 1. Correction de texte

```typescript
async function correctText(text: string): Promise<string> {
  const prompt = `
Corrige les erreurs d'orthographe, de grammaire et de syntaxe dans le texte suivant.
Pr√©serve le style et le ton original.
Ne modifie que les erreurs √©videntes.

Texte √† corriger :
"${text}"

Texte corrig√© :`;

  const result = await generativeModel.generateContent({
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    generationConfig: {
      temperature: 0.1, // Tr√®s faible pour coh√©rence
      maxOutputTokens: Math.max(text.length * 2, 256),
    },
    safetySettings,
  });

  return result.response.candidates[0].content.parts[0].text;
}
```

#### 2. R√©sum√© intelligent

```typescript
async function summarizeText(text: string, maxLength: number = 200): Promise<string> {
  const prompt = `
R√©sume le texte suivant en maximum ${maxLength} mots.
Conserve les points cl√©s et les informations essentielles.
Utilise un style clair et concis.

Texte √† r√©sumer :
"${text}"

R√©sum√© (max ${maxLength} mots) :`;

  const result = await generativeModel.generateContent({
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    generationConfig: {
      temperature: 0.3,
      maxOutputTokens: Math.min(maxLength * 2, 512),
    },
    safetySettings,
  });

  return result.response.candidates[0].content.parts[0].text;
}
```

#### 3. Traduction contextuelle

```typescript
async function translateText(text: string, targetLanguage: string): Promise<string> {
  const prompt = `
Traduis le texte suivant vers ${targetLanguage}.
Pr√©serve le style, le ton et les nuances.
Adapte les expressions idiomatiques si n√©cessaire.

Texte √† traduire :
"${text}"

Traduction en ${targetLanguage} :`;

  const result = await generativeModel.generateContent({
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    generationConfig: {
      temperature: 0.2,
      maxOutputTokens: text.length * 2,
    },
    safetySettings,
  });

  return result.response.candidates[0].content.parts[0].text;
}
```

#### 4. Optimisation de contenu

```typescript
async function optimizeContent(text: string, purpose: string): Promise<string> {
  const prompt = `
Optimise le texte suivant pour ${purpose}.
Am√©liore la clart√©, l'impact et la lisibilit√©.
Conserve le message principal tout en am√©liorant la forme.

Objectif : ${purpose}
Texte original :
"${text}"

Texte optimis√© :`;

  const result = await generativeModel.generateContent({
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    generationConfig: {
      temperature: 0.4, // Plus cr√©atif pour optimisation
      maxOutputTokens: text.length * 2,
    },
    safetySettings,
  });

  return result.response.candidates[0].content.parts[0].text;
}
```

### Gestion d'erreurs

```typescript
async function callGeminiWithRetry<T>(
  operation: () => Promise<T>,
  maxRetries: number = 3
): Promise<T> {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      const isRetryable = error.status === 429 || // Rate limit
                         error.status === 503 || // Service unavailable
                         error.status >= 500;    // Server errors

      if (!isRetryable || attempt === maxRetries) {
        throw error;
      }

      // Backoff exponentiel
      const delay = Math.pow(2, attempt) * 1000;
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}
```

## API Text Embeddings - Recherche s√©mantique

### G√©n√©ration d'embeddings

```typescript
import { TextEmbeddingInput } from '@google-cloud/vertexai';

async function generateEmbeddings(texts: string[]): Promise<number[][]> {
  const model = vertexAI.getGenerativeModel({
    model: 'text-embedding-004',
  });

  const requests: TextEmbeddingInput[] = texts.map(text => ({
    content: text,
    task_type: 'RETRIEVAL_DOCUMENT', // ou RETRIEVAL_QUERY pour questions
  }));

  const result = await model.batchEmbedContents({
    requests,
  });

  return result.embeddings.map(embedding => embedding.values);
}
```

### Chunking intelligent de documents

```typescript
function chunkDocument(text: string, maxChunkSize: number = 500): string[] {
  // Nettoyage du texte
  const cleanText = text.replace(/\s+/g, ' ').trim();
  
  // Division par paragraphes
  const paragraphs = cleanText.split(/\n\s*\n/);
  const chunks: string[] = [];
  let currentChunk = '';

  for (const paragraph of paragraphs) {
    if (currentChunk.length + paragraph.length <= maxChunkSize) {
      currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
    } else {
      if (currentChunk) {
        chunks.push(currentChunk);
      }
      
      // Si le paragraphe est trop long, le diviser par phrases
      if (paragraph.length > maxChunkSize) {
        const sentences = paragraph.split(/[.!?]+/).filter(s => s.trim());
        let sentenceChunk = '';
        
        for (const sentence of sentences) {
          if (sentenceChunk.length + sentence.length <= maxChunkSize) {
            sentenceChunk += (sentenceChunk ? '. ' : '') + sentence.trim();
          } else {
            if (sentenceChunk) {
              chunks.push(sentenceChunk + '.');
            }
            sentenceChunk = sentence.trim();
          }
        }
        
        if (sentenceChunk) {
          chunks.push(sentenceChunk + '.');
        }
        currentChunk = '';
      } else {
        currentChunk = paragraph;
      }
    }
  }

  if (currentChunk) {
    chunks.push(currentChunk);
  }

  return chunks;
}
```

### Recherche par similarit√© cosinus

```typescript
function cosineSimilarity(vecA: number[], vecB: number[]): number {
  const dotProduct = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
  const magnitudeA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0));
  const magnitudeB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0));
  
  return dotProduct / (magnitudeA * magnitudeB);
}

async function findSimilarChunks(
  query: string,
  documentEmbeddings: Array<{ text: string; embedding: number[] }>,
  topK: number = 5
): Promise<Array<{ text: string; similarity: number }>> {
  // G√©n√©rer embedding de la requ√™te
  const queryEmbedding = await generateEmbeddings([query]);
  const queryVector = queryEmbedding[0];

  // Calculer similarit√©s
  const similarities = documentEmbeddings.map(doc => ({
    text: doc.text,
    similarity: cosineSimilarity(queryVector, doc.embedding),
  }));

  // Trier et retourner top K
  return similarities
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, topK);
}
```

## Pipeline RAG complet

### Indexation de documents

```typescript
async function indexDocument(
  documentId: string,
  content: string,
  firestore: FirebaseFirestore.Firestore
): Promise<void> {
  // 1. Chunking
  const chunks = chunkDocument(content);
  
  // 2. G√©n√©ration embeddings
  const embeddings = await generateEmbeddings(chunks);
  
  // 3. Stockage Firestore
  const batch = firestore.batch();
  
  chunks.forEach((chunk, index) => {
    const chunkRef = firestore
      .collection('embeddings')
      .doc(`${documentId}_chunk_${index}`);
    
    batch.set(chunkRef, {
      documentId,
      chunkIndex: index,
      text: chunk,
      embedding: embeddings[index],
      createdAt: new Date(),
    });
  });
  
  await batch.commit();
}
```

### R√©ponse contextualis√©e

```typescript
async function answerWithContext(
  question: string,
  firestore: FirebaseFirestore.Firestore
): Promise<string> {
  // 1. Recherche chunks pertinents
  const queryEmbedding = await generateEmbeddings([question]);
  
  // 2. R√©cup√©ration de tous les embeddings (en production, utiliser une vraie vector DB)
  const embeddingsSnapshot = await firestore.collection('embeddings').get();
  const documentEmbeddings = embeddingsSnapshot.docs.map(doc => ({
    text: doc.data().text,
    embedding: doc.data().embedding,
  }));
  
  const relevantChunks = await findSimilarChunks(
    question,
    documentEmbeddings,
    3 // Top 3 chunks
  );
  
  // 3. Construction du contexte
  const context = relevantChunks
    .map(chunk => chunk.text)
    .join('\n\n');
  
  // 4. Prompt contextualis√©
  const prompt = `
R√©ponds √† la question suivante en te basant uniquement sur le contexte fourni.
Si l'information n'est pas dans le contexte, dis "Je ne trouve pas cette information dans les documents fournis."

Contexte :
${context}

Question : ${question}

R√©ponse :`;

  const result = await generativeModel.generateContent({
    contents: [{ role: 'user', parts: [{ text: prompt }] }],
    generationConfig: {
      temperature: 0.1, // Tr√®s factuel
      maxOutputTokens: 512,
    },
    safetySettings,
  });

  return result.response.candidates[0].content.parts[0].text;
}
```

## Optimisations et bonnes pratiques

### 1. Cache et performances

```typescript
// Cache en m√©moire pour embeddings fr√©quents
const embeddingCache = new Map<string, number[]>();

async function getCachedEmbedding(text: string): Promise<number[]> {
  if (embeddingCache.has(text)) {
    return embeddingCache.get(text)!;
  }
  
  const embedding = await generateEmbeddings([text]);
  embeddingCache.set(text, embedding[0]);
  
  // Limite du cache
  if (embeddingCache.size > 1000) {
    const firstKey = embeddingCache.keys().next().value;
    embeddingCache.delete(firstKey);
  }
  
  return embedding[0];
}
```

### 2. Monitoring et m√©triques

```typescript
import { performance } from 'perf_hooks';

async function monitoredVertexAICall<T>(
  operation: string,
  fn: () => Promise<T>
): Promise<T> {
  const start = performance.now();
  
  try {
    const result = await fn();
    const duration = performance.now() - start;
    
    console.log({
      operation,
      duration: `${duration.toFixed(2)}ms`,
      status: 'success',
    });
    
    return result;
  } catch (error) {
    const duration = performance.now() - start;
    
    console.error({
      operation,
      duration: `${duration.toFixed(2)}ms`,
      status: 'error',
      error: error.message,
    });
    
    throw error;
  }
}
```

### 3. Gestion des quotas

```typescript
class VertexAIRateLimiter {
  private requests: number[] = [];
  private readonly maxRequestsPerMinute = 60;

  async waitIfNeeded(): Promise<void> {
    const now = Date.now();
    
    // Nettoyer les requ√™tes anciennes
    this.requests = this.requests.filter(time => now - time < 60000);
    
    if (this.requests.length >= this.maxRequestsPerMinute) {
      const oldestRequest = Math.min(...this.requests);
      const waitTime = 60000 - (now - oldestRequest);
      
      if (waitTime > 0) {
        await new Promise(resolve => setTimeout(resolve, waitTime));
      }
    }
    
    this.requests.push(now);
  }
}
```

## Exemples d'utilisation avanc√©e

### Prompt Engineering pour diff√©rents styles

```typescript
const STYLE_PROMPTS = {
  formal: "Utilise un langage soutenu et professionnel",
  casual: "Utilise un ton d√©contract√© et accessible", 
  technical: "Utilise un vocabulaire technique pr√©cis",
  creative: "Sois cr√©atif et original dans l'expression",
};

async function styleText(text: string, style: keyof typeof STYLE_PROMPTS): Promise<string> {
  const styleInstruction = STYLE_PROMPTS[style];
  
  const prompt = `
${styleInstruction}.
R√©√©cris le texte suivant en conservant le sens mais en adaptant le style :

"${text}"

Texte r√©√©crit :`;

  return await callGemini(prompt, { temperature: 0.6 });
}
```

### Analyse de sentiment avec contexte

```typescript
async function analyzeSentiment(text: string): Promise<{
  sentiment: 'positive' | 'negative' | 'neutral';
  confidence: number;
  explanation: string;
}> {
  const prompt = `
Analyse le sentiment du texte suivant et r√©ponds au format JSON :
{
  "sentiment": "positive|negative|neutral",
  "confidence": 0.85,
  "explanation": "Explication courte"
}

Texte √† analyser :
"${text}"

Analyse JSON :`;

  const result = await callGemini(prompt, { temperature: 0.1 });
  return JSON.parse(result);
}
```

Ce guide couvre les aspects essentiels de Vertex AI pour Magic Button. Pour des cas d'usage plus sp√©cifiques, consultez la documentation officielle Vertex AI et adaptez les exemples √† vos besoins.